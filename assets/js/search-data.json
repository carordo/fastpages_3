{
  
    
        "post0": {
            "title": "Data Analysis and Prediction of Waterpoint Functionality Using  Spatial and Non-spatial Machine Learning Models",
            "content": "Data Analysis and Prediction of Waterpoint Functionality Using  Spatial and Non-spatial Machine Learning Models . Georgia Tech OMSA Practicum with ESRI . Carlos Ordonez, Rozsa Zaruba . December 1st, 2021 . Abstract . The main purpose of this project is to apply the knowledge obtained during the Master’s in Analytics by way of using ESRI’s ArcGIS Pro spatial software and their Python API, ArcPy. The learning process was through a project that analyses water point data in Uganda. With ESRI’s team guidance, we were set on a learning adventure. We learned how to manipulate our data in ArcGIS, how to transfer the code into Jupiter notebooks, how to think spatially, and use layers to better extract information. We were able to see the differences (both advantages and disadvantages) between traditional Python programming and ArcPy. At the end, we used our knowledge to build classification models to be able to tell if a water point is functional or not. Finally, we used ESRI’s story map to create a presentation of our findings. . The interactive map can be found here: Uganda Water Point Analysis - Storymaps . Introduction . Having clean and safe water, is a basic necessity that we take for granted. Unfortunately, many rural areas in the World have no easy access to water. Most of Uganda’s population live in rural areas and two third of them lack easy access to safe water. Women and children spend most of their day walking back and forth between water source and home. . People in many developing worlds walk an average of 3.7 miles a day for water. Some parts in Uganda if people want to access clean water, the only reliable borehole is about this distance one way. It can take between 90 minutes to two hours to walk that distance. Boreholes sometime serve more than 850 people, which makes them incredibly crowded. You could wait two to three hours because of long lines . This has an impact on everybody’s life: women cannot get jobs, children cannot go to school, etc. Having water access does not guarantee that is safe to consume, creating a health impact to society. The lack of infrastructure and access to clean potable water in Uganda is one of the factors slows their progress1 . Problem Statement . This project analyses the water point data set for Uganda from the Water Point Data Exchange (WPDx) using ESRI’s ArcGIS Pro and their Python API. The project aims to find relationships in the dataset that might explain why certain places do not have access to water, with the final goal of finding a model that can predict the functionality of a water points. . Data . Water Point Data Exchange (WPDx)2 where waterpoint data are a set of records that uniquely identify water points and provide additional attributes such as the type and condition of the infrastructure, functionality, service levels, etc. . | ESRI provided additional data sets: . World Population Estimate for 2016 . | Africapolis urban area dataset . | Enrichment data for Uganda from arcgis.geoenrichment3 . | | Data Cleaning . Our original dataset had 121,472 observations on 52 features. The provided dataset for Uganda contained points outside the country’s boundaries. After removing the points outside, the total observation remaining were 118,110. The dataset also contained many redundant columns: e.g. “management_clean”, “management_original” and “management”, containing the same data. We kept the ones we thought would be most informative for our study. . Examining the data further, we noticed that more than 80% of the data in the “water_tech_clean” was labeled as “NaN”. It was decided not to impute data for this attribute in our study, because most of the variables are categorical, and all of them have multiple levels (management has 137 levels) and it would muddy our results. Our solution was to remove the missing values. For “water_tech”, that would mean reducing our data drastically. Reading up on the variables, we realized that “NaN” is the base line for this variable, and it means that there is no technology in place. E.g., there is a river and people would just use their jugs to fill it up with water. Our solution was to change the variables to “Notech” value and use it as one of the levels in the variable. . Taking advantage of ArcGIS Pro, we were able to add a new categorical variables to our dataset such as “Type_of_Region” describing if the point observation is considered to be in an urban or rural area. . ## . Data Exploration: . For the data exploration portion, we are going to answer questions as provided in the scope of the project to try to understand more about the data. . Which district has the most non-functional water points? | . For the project, it was determined that ”F_status_id” provides information about the functionality of the water point. For this question, the approach is to work with non-functional data and obtain the districts in percentage with the largest number of non-functional water points. . Figure 1. Heatmap of non-functional water points per district as percentage of the total non-functional waterpoints, being the district of Wakiso the largest with a total of 930 or 4.36% of the total non-functional. . . Figure . Heatmap of non-functional water points per district as percentage of the total non-functional waterpoints . . Figure . Heatmap of non-functional water points as percentage of total waterpoints per district . Figure 2 shows the percentage of non-functional waterpoints as a portion of the total functional and non-functional waterpoints per districts having the district of Pakwach the largest with 44.08% being non-functional as per Figure 3. . . Figure . Top 5 Districts with most non-functional waterpoints per district totals . What are the top causes of non-functional water points? | . Analyzing the causes of non-functional water points the initial approach was to understand what type of labeling the attribute “F_status” had on each point. Table 1 shows that 90.76% has “Non-functional” label. . . Exploring further, it was found that out of the 18,614 non-functional water points, there are 14,914 “Community Management” equivalents to 80.12% of total non-functionals. Furthermore 3,973 of those non-functional charged a fee. It is possible to assume that the community managing the water points does not have the means to be able to support the infrastructure necessary to treat them, and the Government cannot reach those communities in order to provide a service in which they could charge. Also, it is possible that the poverty level of those communities does not allow them to pay for water service. . Which year saw the most water point installations? | . Doing a bar plot analysis as per Figure 4 it was found that 2007 was the year with the highest water point installations. There is a clear increase after 1986, which makes sense if we consider the country’s history. Uganda had a Civil War between 1980 till 1986. People started going back to their homes, there was need for more water, and the country tried to recover from the war. . . . Figure Number of installations per year . Which water points serve the largest rural population? Repeat the analysis for urban population. Do these water points have anything in common? | . Using spatial filtering techniques with “Uganda_UrbanAreas_Africopolis” as the filter, a new attribute was created to determine rural and urban water points. Then with the layer “Uganda_Pop2016_points_clipped” a boundary of 1km was established outside the urban areas. We established a large population by finding the mean of the population, that was established at 436 per population point as shown on Figure 5. Figure 17 in the Appendix shows the distribution of the population and the mean. . . Figure . Waterpoints serving the largest urban and rural populations . Is there a relation between age of water point and the water point being Non-functional? | . When we are looking at the summary of the data for functional and non-functional water points based on the installation year, we get a false sense that in recent installation years we have more broken points. In reality, there are more water points installed recently than in the past. We divided up the data into two intervals: water points installed before 1985 we consider old, everything after is new. Based on the histogram 1985 looked like a natural break, new point installations picking up after. As we mentioned above, this aligns with the country’s history too. . We calculated the percentage of non-functional water points for the old and new installations. We found that 27.6% of the old water points were non-functional, while “only” 18.5% of the new wells were broken. To answer the question, yes the older the water point it is more likely that it may be broken. . . Figure Installation per year with functionality . Which district has the largest number of old water points serving the highest rural population? | . After aggregating the points by districts for the largest water points of the rural areas, the district of Ntungamo shows the largest number of points. The graph shows the values in percentages. Ntungamo is the green district in the south of the country in Figure 7. . . Figure Heatmap of districts with largest old waterpoints serving highest rural populations . The top five districts with largest old waterpoints serving the highest rural populations are shown on Figure 8 . . Figure Heatmap of top 5 districts with largest old water points serving highest rural populations . Is there an association between water source, water tech, facility type or management for Non-Functional water points? | . Using aggregate numbers for each of these variables can be misleading. For example, boreholes have the greatest number of non-functioning water points. But most of the water points are boreholes. We decided to use percentages again to see the relationships better. Our Findings: . As a result, 20.85 % of the “undefined shallow wells are broken. The number of “protected shallow wells” and “piped water” are much smaller to fairly compare to the boreholes, but it is still alarming that 23% of them is broken . | 23% of the hand pumps are broken . | Only 17% of the water points is broken if the facility type is improved. It jumps up to 29% for unknown or 24% if there is no facility. . | Private operators have only 7% of broken water points. Community management and Other Institutional Management are doing fairly with 19% and 25% broken points. All the other managements do poorly maintaining the water points, school management being the worst with 81% of broken water points. . | . Another way to look at association between variables is to look at the correlation matrix as per Figure 9. The correlation matrix does not show high correlations. To see how significant the relationships are we did a chi-square analysis with the following null and alternative hypothesis: . H_0: There is no association between pairwise variable. . H_Alt: There is significant association. . Our p-values are very large, which tells us that we cannot reject the null hypothesis, there is no significant association between variables. . . Figure Correlation matrix of selected attributes . We have to note that the correlation matrix and chi-square analysis both assume linear relationship. Our conclusion thus is that there is no linear association between variables, we need to find models that handle non-linear relationships (like Random Forest) and avoid the ones that assume linear relationships (like Logistic Regression). . What percent of the population does not have easy access to working water points? | . This question was answered entirely using ArcPy and ArcGIS Pro. We created 1 km buffers and counted all the points that fall outside of it as shown in Figure 10. The total number of population outside of a 1km radius of functional water points is 7,010,728. This is equivalent to almost 20% of Uganda’s population (for the year of 2016). . . Figure 1Km buffer in blue to aggregate counts of waterpoints outside the border . GeoEnrichment and Imputation . Our results in the previous sections did not show a clear relationship between variables. To improve our dataset, we decided to enrich the data. We connected with the online ArcGIS tool to enrich the data for every district. One of the challenges that we encountered is that the enriched data had fewer districts than the one in our dataset. This created a series of null values in several districts. To overcome this, we impute information based on the location of the district. This was accomplished by finding the districts in the vicinity of the missing district and taking the average of the enriched parameters. Figure 11 shows in darker colors the districts that had no enrichment information and the districts that were used for imputation around them in a lighter color. . . Figure Districts in dark have requires impute with neighbor districts in lighter color . The KeyFacts in the GeoEnrichment library contained potentially valuable information for the country and the districts. Some of the available variables were unemployment, population density, number of males, number of females, etc. . The enriched data showed promising attributes, but many of them were correlated with each other. By removing the highly correlated attributes and leaving the ones that explain the data better, a new dataset was created with the following variables: . F_lat_deg Latitude information (used in spatial analysis) . F_lon_deg | Longitude information (used in spatial analysis) | . F_status_id | Yes/No Working and non-working water points | . F_water_source_clean | Categorical: 13 unique values | . F_facility_type | Categorical: improved/unimproved | . F_install_year | Installation year | . management_clean | Categorical: reduced to 9 unique values | . water_source_category | Categorical: reduced to 7 unique values | . water_tech_category | Categorical: 4 unique values | . Type_of_region | 0/1: Rural/Urban | . Age | old/new | . ADM1_EN | Eastern/ Northern/ Central/ Western | . UNEMP_CY | Unemployment from key facts | . POPPRM_CY | Population per million | . AVGHHSZ_CY | Average household size | . Table Causes of non-functionality . Feature Extraction . As we saw above, it is hard to find a meaningful explanation between variables. It does not mean that there is none, it only means that there is no linear association. Clustering would find points that are similar to each other, so it is a good method for feature extraction. We explore a couple of techniques and save results as features for our classification analysis. . Our non-spatial clustering method is KModes clustering. It is one of the unsupervised Machine Learning algorithms that is used to cluster categorical variables (with categorical variables we cannot calculate the distance as we would do with Kmeans algorithm). This method uses the dissimilarities (total mismatches) between the data points. The lesser the dissimilarities the more similar our data points are. It uses modes instead of means. . The KModes images show points with similarities. To the naked eye it is difficult to assess what similarities they have. Our intent with this clustering technique is to use it for modeling as features (variable in the dataset is Cluster_KModes). . . Figure KModes spatial representation . We explored three different spatial clustering techniques such as: Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN), Ordering Points To Identify Cluster Structure (OPTICS), and Spatial Constrained Multivariate Clustering. . HDBSCAN is a powerful algorithm that uses Density-Based Spatial Clustering of Applications with Noise (DBSCAN), converting it into a hierarchical clustering algorithm, and then using a technique to extract a flat clustering-base in the stability of clusters. Densities are estimated based on core distances and form the density landscape. Using a global threshold to set the sea level at, DBSCAN identifies the “islands“ (DBSCAN). Then HDBSCAN helps decide: are these several “islands” or one “island” with multiple peaks? Figure 13 shows a representation of HDBSCAN clusters and the output was saved as a feature for modeling. The gray color represent noise. Figure 19 shows cluster allocation. . . Figure Clustering using HDBSCAN . OPTICS is a technique that has two principles: Core Distance and Reachability Distance. Core Distance is defined as the minimum value of radius required to classify a given point as a core point. If the given point is not a Core point, then its Core Distance is undefined. The Reachability Distance: is defined with respect to another data point. This clustering technique is different from other clustering techniques as it does not explicitly segment the data into clusters. Instead, it produces a visualization of Reachability distances and uses this visualization to cluster the data. The resulting clusters are visualized in Figure 14. The blue cluster represents the large allocation of a cluster. Figure 20 shows the cluster allocation. Figure 21 shows the reachability plot. After further examination we noticed that HDBSCAN and OPTICS have a 99.5% correlation. For our future analysis we decided not to use the OPTICS results as it would not add extra information to our models. . . Figure Clustering using OPTICS . Spatial Constrained Multivariate Clustering: This is an unsupervised machine learning method that finds spatially contiguous clusters of features based on a set of attribute values and optional cluster size limits. Figure 15 shows the result. Our interpretation is that points are being cluster by latitude and longitude and installed year. The goal of using this method is to help our algorithm determine which algorithm help improve our modeling. . . Figure Spatially Constrained Multivariate Clustering . Dimensionality Reduction . Dimensionality reduction helps compress the data with less noise. We can use the reduced data for our modeling. Two methods were explored: Multiple Correspondence Analysis and Autoencoder Neural Network. . Correspondence Analysis (CA) is a Dimensionality Reduction technique that is traditionally applied to contingency tables. It transforms data in a comparable manner as Principal Component Analysis (PCA), where the central result is a Singular Value Decomposition (SVD). CA needs the data to be in contingency tables which means it is more appropriate to apply it to categorical features. Multiple Correspondence Analysis (MCA) is an extension of the CA for more than two categorical features (three or more). We have more than two categorical features, hence the use of MCA. We ran MCA on our data reducing the features to the first 2 components. . Autoencoder is an unsupervised artificial neural network that compresses the data to lower dimensions and then reconstructs the input back. Autoencoder finds the representation of the data in a lower dimension (middle layer) by focusing more on the important features getting rid of noise and redundancy. We are going to use this middle, compressed layer as features in our modeling. . Data used for modeling: . For most of our analysis, we used the reduced enriched dataset with clustering information. We also explored a dataset with only the MCA components and clustering information, another with only autoencoder features and clustering information. For more details see Appendix. . Proposed Methodology for Analysis . The main question that we would like to answer is: Can we predict the status of a water point (working or non-working)? This is a classification problem. Classification usually refers to any kind of problem where a specific type of class label is the result to be predicted from the given input field of data. . We approached the problem in two ways: Through spatial analysis and non-spatial analysis. For non-spatial analysis, KNN, Naïve Bayes (NB), SVM, Random Forest are used from Python’s scikit-learn library. For spatial analysis, we use Random Forest Classifier from ArcPy package. . To have a base reference we are running a dummy classifier (only identifies majority class labels), but we are also using Naive Bayes as a baseline. It does not make sense to do a logistic regression as the data exploration did not suggest a linear relationship. Our main focus is going to be on K-Nearest Neighbor (KNN), Support Vector Machines (SVM) and Random Forest models. . *Naïve Bayes* (NB) classifier is a probabilistic classifier based on Bayes’ theorem, which assumes that each feature makes an independent and equal contribution to the target class. NB classifier assumes that each feature is independent and does not interact with each other, such that each feature independently and equally contributes to the probability of a sample belonging to a specific class. . K-Nearest Neighbor (KNN) is a supervised and pattern classification learning algorithm that helps find which class the new input belongs to when k nearest neighbors are chosen and distance is calculated between them. It attempts to estimate the conditional distribution of Y given X, and classify a given observation to the class with the highest estimated probability. We found the optimal number of neighbors using the elbow method. Each model for each dataset was tuned individually and the number of k that minimized error rate was chosen for each model as shown in Figure 18. . Support vector machine (SVM) is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: the algorithm creates a line or a hyperplane which separates the data into classes. For our data this may not be the best solution as this method cannot create nonlinear separations. . Random Forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. We are using grid search to find the optimal setup for each dataset. The decision metric for the grid search is ‘ROC-AUC’. . Metrics Used for Evaluation . Confusion Matrix is a performance measurement of the classification. . . Figure Confusion Matrix Illustration . The Kappa Coefficient: Cohen’s Kappa is defined as (Observed Accuracy - Expected Accuracy) / (1 - Expected Accuracy). It evaluates how well the classification performs compared to chance, in which all values are just randomly assigned. The coefficient ranges from -1 to 1 and a value greater than 0 indicates that the classification is significantly better than random. This metric was used between non-spatial models. . Precision — Measures what percent of your predictions were correct. Precision can be defined as the ability of a classifier not to label an instance positive that is actually negative. For each class, it is defined as the ratio of true positives to the sum of a true positive and false positive. . Recall — Measures what percent of the positive cases did you catch. Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives. . F1 score — Measures what percent of positive predictions were correct. The F1 score is a weighted harmonic mean of Precision and Recall such that the best score is 1.0 and the worst is 0.0. F1 scores are lower than accuracy measures as they embed Precision and Recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy. . AUC (Area Under the ROC curve) — Represents the degree or measure of separability. It tells how much the model can distinguish between classes. The higher the AUC, the better the model is at predicting classes correctly. AUC is scale-invariant: it measures how well predictions are ranked, rather than their absolute values. Also, AUC is classification-threshold-invariant: It measures the quality of the model’s predictions irrespective of what classification threshold is chosen. . Dealing With Unbalanced Data . Before starting the modeling, we examined the data for class distribution, and we noticed that the two classes are heavily unbalanced: only 19% of the data belongs to the minority class (non-functional water points). The challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important. Some of the classical solutions would be to use methods that train on the majority class and identify minority classes as an anomaly. Two methods were tried in our study: . One-Class SVM: When modeling one class, the algorithm captures the density of the majority class and classifies examples on the extremes of the density function as outliers. . | Isolation Forest is a tree-based anomaly detection algorithm. It is based on modeling the normal data in such a way to isolate anomalies that are both few in number and different in the feature space. Tree structures are created to isolate anomalies: isolated examples have a relatively short depth in the trees, whereas normal data is less isolated and has a greater depth in the trees. . | Having unbalanced data made us consider different over and under-sampling techniques too. We are creating data sets with over-sampling, under-sampling and over-and-under sampling at the same time. We kept a separate testing set and only over/under-sample the training data. We train the models on this data and use the separate testing data for testing. . Oversampling Techniques . One approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples do not add any new information to the model. We call this traditional oversampling. . Synthetic Minority Oversampling Technique (SMOTE) synthesizes new examples from the existing examples. There are variations of this approach: . SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space, and drawing a new sample at a point along that line. Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example is found (typically k=5). A randomly selected neighbor is chosen, and a synthetic example is created at a randomly selected point between the two examples in feature space. . | Borderline-SMOTE is an extension to SMOTE, it involves selecting those instances of the minority class that are misclassified, such as with a k-nearest neighbor classification model. We can then oversample just those difficult instances, providing more resolution only where it may be required. . | Borderline-SMOTE SVM: Instead of k-nearest neighbor, it uses an SVM. Also, the technique attempts to select regions where there are fewer examples of the minority class and tries to extrapolate towards the class boundary. . | . Under-sampling Methods . Under-sampling is a technique to balance uneven datasets by keeping all of the data in the minority class and decreasing the size of the majority class. We tried the following variations: . NearMiss-1: Selects only those majority class examples that have a minimum distance to three minority class instances, defined by the n_neighbors argument. . | Tomek Links: If the examples in the minority class are held constant, the procedure can be used to find all of those examples in the majority class that are closest to the minority class, then removed. . | Edited Nearest Neighbors Rule for Undersampling: This rule involves using k=3 nearest neighbors to locate those examples in a dataset that are misclassified and that are then removed before a k=1 classification rule is applied. . | One-Sided Selection for Undersampling: It is an undersampling technique that combines Tomek Links and the Condensed Nearest Neighbor (CNN) Rule. . | Neighborhood Cleaning Rule for Undersampling: It combines both the Condensed Nearest Neighbor (CNN) Rule to remove redundant examples and the Edited Nearest Neighbors (ENN) Rule to remove noisy or ambiguous examples. It focuses less on improving the balance of the class distribution and more on the quality (unambiguity) of the examples that are retained in the majority class. . | . Random Over-and-under Sampling . These techniques use both over and undersampling together to create the training dataset. First, they are oversampled and undersampled right after. Variations that we used: . Random over than undersampling . | SMOTE-TOMEK . | SMOTE and Edited Nearest Neighbors . | . Analysis and Results . Summary of Non-Spatial Results . We ran many models with different data sets and are comparing the best of them. More detailed outputs can be found in the Appendix. . Our dummy classifier (a classifier that only finds the majority label) has an average accuracy of 81%. This is already a sign why not trust this metric since we already know that it does not capture any of the minority cases. The AUC curve is 0.5, the Cohen Kappa is 0, telling us that this is not a useful model. . Our other base model is the Naïve Bayes: the best model we found was for the sample that was SMOTE oversampled and Nearest Neighbor undersampled at the same time. AUC curve improved to 0.58, Cohen Kappa 0.103 which suggests that the model is somewhat useful. We also look at the precision and Recall numbers, especially we are interested in the minority class. With a higher Recall but low Precision, our classifier considers that more points are non-functional than they are. . Support Vector Machine: We were getting some promising Precision results (see Table 4), but the Recalls were very much close to 0. If we did not care about false negatives, and we only focused on true positives and false positives this high Precision result may even be good. But this is another question because we do not have a cost analysis to associate to the model. Which one costs more? Going out to repair a wrongly classified working well, or not going out to a non-working well? Models need high Recall when we need output-sensitive predictions and we need to cover false negatives as well. If it is okay to flag working wells as not working but a non-working wells should not be flagged as working then we would need a high Recall. The F-1, Cohen Kappa and AUC on the other hand suggested that this model is not good. . Table 5 largest waterpoints installation per year . **Sample Autoencoder and clusters** . precision recall f1-score support . 0 0.80 0.00 0.01 4941 . 1 0.81 1.00 0.89 20486 . accuracy 0.81 2542 . macro avg 0.80 0.50 0.45 25427 . weighted avg 0.80 0.81 0.72 25427 . Cohen Kappa: 0.005 . AUC: 0.502 . . Table 5 largest waterpoints installation per year . Adding the cluster information to our base enriched data did not improve significantly our models. Same for using MCA and Autoencoder Neural Network reduced datasets. One-class classification methods did not seem to work better either. We started seeing some changes with over/under sampling techniques. We used the enriched dataset with cluster information to create new samples on the training data. We ran most of the models (except for the one-class models) and saw that the AUC moved above 0.60, the precision and Recalls are closer to each other. We chose the traditional oversample results (Table 5) to be able to compare it with the spatial analysis and because it got slightly better results. The rest of the sampling methods created synthetic data and we cannot create coordinate data with them to be able to use it in spatial analysis. . Table List of selected attributes after enrichment . Model . Sample traditional oversample . Precision Recall F-1 Cohen Kappa AUC Accuracy . Status_id | 0 | 1 | 0 | 1 | 0 | 1 | | | | . KNN k=3 | 0.56 | 0.79 | 0.55 | 0.8 | 0.55 | 0.79 | 0.347 | 0.672 | 0.72 | . Naïve Bayes | 0.3 | 0.81 | 0.05 | 0.97 | 0.08 | 0.88 | 0.029 | 0.51 | 0.79 | . Support Vector Machine | 0.65 | 0.74 | 0.28 | 0.93 | 0.39 | 0.82 | 0.249 | 0.606 | 0.72 | . Random Forest | 0.66 | 0.79 | 0.5 | 0.88 | 0.57 | 0.83 | 0..404 | 0.689 | 0.76 | . Table List of selected attributes after enrichment . The two most promising models were the K-Nearest Neighbor and Random Forest classifier. Both models performed well on almost all of the data samples. For the traditional oversample data both had an AUC above 0.65 (67% and 69% respectively), with the highest Precision on the minority class of 66% achieved with Random Forest. Again, it was interesting to see, that if we were to choose a model based on average accuracy, Naïve Bayes would have been chosen. We can see that all the other metrics suggest that NB model is not that good. . Random Forest was the most stable model for most of the data, so we are reporting all of the results in Table 6. It is interesting to note that with the neighborhood type under-sampling methods, we were able to create models that had high AUC, and at the same time high Recalls on the minority class (0.79). Again, if our goal is to not miss any of the non-working points (our false negatives to be as low as possible) and we don’t care that some of them are actually working water points, we can use these models. We have to be careful using the F1 as the metric for comparing the models as it gives equal weight to Recall and Precision. If we know that one or the other is more important in our analysis, we can use a weighted version of the metric that gives different weights for the two metrics. . Full Random Forest Results on All the Sampling Methods . Model Precision   Recall   F-1   Cohen Kappa AUC Accuracy . Status_Id | 0 | 1 | 0 | 1 | 0 | 1 |   |   |   | . Sample enriched | 0.65 | 0.93 | 0.15 | 0.98 | 0.25 | 0.9 | 0.189 | 0.567 | 0.82 | . Sample enriched with clusters | 0.64 | 0.83 | 0.16 | 0.98 | 0.26 | 0.9 | 0.198 | 0.571 | 0.82 | . Sample MCA and clusters | 0.61 | 0.83 | 0.15 | 0.98 | 0.24 | 0.9 | 0.177 | 0.563 | 0.82 | . Sample Autoencoder and clusters | 0.62 | 0.82 | 0.12 | 0.98 | 0.21 | 0.9 | 0.152 | 0.553 | 0.82 | . Sample traditional oversampling | 0.66 | 0.79 | 0.5 | 0.88 | 0.57 | 0.83 | 0.404 | 0.689 | 0.76 | . Sample x1,y1 SMOTE | 0.35 | 0.87 | 0.53 | 0.76 | 0.42 | 0.81 | 0.248 | 0.649 | 0.72 | . Sample X3,y3 Borderline SMOTE | 0.35 | 0.87 | 0.53 | 0.76 | 0.42 | 0.81 | 0.247 | 0.648 | 0.72 | . Sample X4,y4 Borderline SMOTE SVM | 0.35 | 0.87 | 0.53 | 0.76 | 0.42 | 0.81 | 0.247 | 0.648 | 0.72 | . Sample X5,y5 NearMiss-1 | 0.21 | 0.82 | 0.62 | 0.42 | 0.31 | 0.56 | 0.025 | 0.522 | 0.46 | . Sample X9,y9 Tomek links | 0.35 | 0.87 | 0.53 | 0.76 | 0.42 | 0.81 | 0.246 | 0.646 | 0.72 | . Sample X10,y10 Edited Nearest Neighbor undersample | 0.27 | 0.9 | 0.79 | 0.47 | 0.4 | 0.62 | 0.151 | 0.632 | 0.53 | . Sample X11,y11 One-Sided Selection | 0.35 | 0.87 | 0.54 | 0.76 | 0.42 | 0.81 | 0.249 | 0.649 | 0.72 | . Sample X12,y12 Neighborhood cleaning rule | 0.29 | 0.9 | 0.73 | 0.57 | 0.41 | 0.7 | 0.189 | 0.649 | 0.6 | . Sample X13, y13 under and oversample | 0.41 | 0.86 | 0.41 | 0.86 | 0.41 | 0.86 | 0.271 | 0.636 | 0.77 | . Sample X14,y14 SMOTE+TOMEK | 0.39 | 0.86 | 0.46 | 0.83 | 0.42 | 0.84 | 0.264 | 0.641 | 0.75 | . Sample X15,y15 SMOTE+Edited Nearest Neighbor | 0.36 | 0.88 | 0.54 | 0.77 | 0.44 | 0.82 | 0.266 | 0.658 | 0.73 | . Table SVM with Autoencoder and clusters . #### . Spatial Analysis Results . In spatial modeling we used Forest-based Classification and Regression on different datasets with 100 trees. Adding clustering data did not seem to improve the models. Model 1 and Model 2 show very similar results. Model 3 and 4 use only the clustering features and MCAs respectively. They perform well for functional water points but underperform for non-functional ones. For Model 5 and 6 we are using additional layers with over and undersampling. Using explanatory training distance features improves the performance of non-functional water points. The best performer is Model 6 with adding the Ug_Wetlands-extent-and-types1996 layer extracted from Africa Geoportal hosted by Esri4 , followed really closely by model 5 with the Uganda_UrbanAreas_Africapolis layer provided with the initial package. Both layers can be seen in Figure 23 . These models had the highest AUC and Precision. . It is interesting to note that the best two models (5 and 6) use two types of techniques to manage the imbalanced data, yet they arrive at very similar results. Model 5 uses oversampling and Model 6 uses undersampling. Figure 24 shows a map of the results of Model 6. In blue correctly classified points and in orange misclassified points. Figure 25 shows the prediction performance plot for Model 6. Figure 26 shows the prediction performance table for Model5. Table 7 provides a summary of the results for each model for spatial modeling. . . Conclusion . Comparison Between Spatial and Non-Spatial Data . After testing different models using enriched data set with ArcGIS Pro’s Random Forest (spatial models) and scikit-learn (non-spatial models), we were able to obtain slightly better results with non-spatial models. This may be because we have more knowledge and tools to tune the non-spatial models. For example, we were able to implement the SMOTE method and other synthetic sampling methods to deal with the unbalanced data. . Working with imbalanced data, we considered the F1, Recall, and Precision scores to be a more accurate representation of the performance of the models. When choosing our model, we are considering higher numbers on the minority labels mentioned above. . Regarding spatial models, we chose Model 6 because it had the best F1, Precision, Recall and AUC on the minority class. Model 5 had very close results with slightly better metrics on the majority class. . In comparison, for the non-spatial models KNN and Random Forest performed best. Random Forest seemed to handle the task slightly better at every sampling method. . We noticed that the spatial analysis produced more consistent results regardless of the sampling method. Non-spatial analysis showed different results based on the sampling method as we mentioned above. If our objective is to have higher Recall, we would suggest Nearest Neighbor Undersampling methods to achieve it. Also, we have to note, that if our goal is high Precision, we would suggest the SVM model. Table 8 shows the summary of the best spatial and non-spatial models. . Model Precision   Recall   F-1   AUC . Status_Id | 0 | 1 | 0 | 1 | 0 | 1 |   | . SPATIAL |   |   |   |   |   |   |   | . Model 5: Layer Africapolis | 0.65 | 0.67 | 0.42 | 0.84 | 0.51 | 0.75 | 0.63 | . Model6: Layer Wet lands | 0.66 | 0.67 | 0.42 | 0.83 | 0.52 | 0.73 | 0.63 | . NON-SPATIAL |   |   |   |   |   |   |   | . Sample traditional oversampling (RF) | 0.66 | 0.79 | 0.5 | 0.88 | 0.57 | 0.83 | 0.689 | . Sample X10,y10 Edited Nearest Neighbor undersample (RF) | 0.27 | 0.9 | 0.79 | 0.47 | 0.4 | 0.62 | 0.632 | . Sample X12,y12 Neighborhood cleaning rule (RF) | 0.29 | 0.9 | 0.73 | 0.57 | 0.41 | 0.7 | 0.649 | . Sample X15,y15 SMOTE+Edited Nearest Neighbor (RF) | 0.36 | 0.88 | 0.54 | 0.77 | 0.44 | 0.82 | 0.658 | . KNN k=3 | 0.56 | 0.79 | 0.55 | 0.8 | 0.55 | 0.79 | 0.606 | . Naïve Bayes | 0.3 | 0.81 | 0.05 | 0.97 | 0.08 | 0.88 | 0.51 | . Dummy Classifier | 0.00 | 0.81 | 0.00 | 1.0 | 0.00 | 0.89 | 0.50 | . Table Non-spatial modeling highlights . Recommendations . Throughout the analysis, we focused on how to predict if a water point is working or not working. Our models for the minority did not perform as desired in predicting them and it made us think that it is almost random which water point is broken and which one is not. As we mentioned at the beginning, for the water technology column, the baseline is “no technology”, and more than 80% of the data had no pump, no kiosk etc. In our dataset only 154 observations were on piped water. The majority was a borehole (36,571), protected spring (27,660), undefined shallow well (18,177) rainwater harvesting (16,620). Assuming that people have access to these water sources, what guarantees that the water is safe to drink? It is a vicious cycle: People cannot afford safe water so they walk hours to get water that might not be potable. The socio-economic impact is dramatic. Because they spend their time going after water, they cannot be employed. Also, they are more likely to get sick from the unhealthy conditions, which would keep them away from work, put a strain on the medical system etc. The real solution would be affordable water through infrastructure, where the quality of the water could be controlled. Strategically build water points within easy reach for everyone. . Lessons Learned . ArcPy and ArcGIS Pro Versus Regular Python: . The main purpose of this project was not only to do data analysis but also to learn how to work with ArcGIS Pro and use ArcPy. We used manly the ArcGIS Pro for both spatial and non-spatial analysis. Our Jupiter notebooks were created inside ArcGIS Pro, we never left this environment. It was easy to see that without the capabilities of ArcGIS Pro we would have missed crucial information. For example, it would have been impossible to distinguish and find the points outside the country’s boundaries. There were many calculations that were at the tip of our fingertip using ArcPy functions, while in regular Python it took several steps and modifications to get the same results. One of the questions we were only able to answer using only ArcPy (creating buffers to be able to calculate how many people were outside easy reach of water as Figure 22). We found the ArcGIS Pro’s GUI was easy and fast to work with, but some calculations when transferred into the notebook using ArcPy took considerable time to run. The under and oversampling was done automatically in ArcGIS while on regular Python we had to create new datasets and then use them in the models. We found that our training from our graduate studies helped a lot in tuning our non-spatial models. . Challenges That We Overcame: . Our first challenge was to quickly understand how to think spatially, how we can use layers on top of each other to extract information. Once learned, we found that some of tasks became very natural and easy. The way we overcome this first challenge was to rewatch lectures, read about the topic and mostly talk it through with each other. It helped a lot when we were coding and debugging together through shared screens. . The rest of the challenges were mostly technical and not about what we needed to do, but how to do it right. For example, improving our data, through enrichment, took considerable time and effort to get the results that we were happy with. It required imputing data to districts with missing data. Also, the enrichment data was district data, we needed point data for within each district. ArcGIS Pro was a big help doing these tasks. . For modeling, we did not want to run every model we were ever thought, but rather use the ones that make sense based on our data exploration and questions (e.g. It would not make sense to run a regression on a classification problem). . References: . Evaluation metrics: . https://medium.com/@klintcho/explaining-precision-and-Recall-c770eb9c69e9 . https://machinelearningmastery.com/roc-curves-and-precision-Recall-curves-for-classification-in-python/ . Sampling methods: . https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/ . https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/ . https://machinelearningmastery.com/roc-curves-and-precision-Recall-curves-for-classification-in-python/ . https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/ . https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/ . Dimensionality reduction: . https://machinelearningmastery.com/autoencoder-for-classification/ . https://blog.keras.io/building-autoencoders-in-keras.html . https://towardsdatascience.com/autoencoder-neural-networks-what-and-how-354cba12bf86 . Dealing with unbalanced datasets: . https://bigdata-madesimple.com/dealing-with-unbalanced-class-svm-random-forest-and-decision-tree-in-python/ https://machinelearningmastery.com/cost-sensitive-svm-for-imbalanced-classification/ https://machinelearningmastery.com/one-class-classification-algorithms/ https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/ https://www.kdnuggets.com/2020/04/introduction-k-nearest-neighbour-algorithm-using-examples.html . Picture sources: . https://jssozi.wordpress.com/2010/10/15/water-and-people-in-rural-uganda/ . https://www.oregonlive.com/travel/2011/04/your_best_shot_children_carryi.html . https://www.worldvision.org/clean-water-news-stories/long-path-clean-water . Books: . Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1, no. 10. New York: Springer series in statistics, 2001. . James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013. . Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg, 2006 . Appendix: . Different datasets for analysis . Enriched base dataset Enriched with clusters . F_lat_deg | F_lat_deg | . F_lon_deg | F_lon_deg | . F_status_id | F_status_id | . F_water_source_clean | F_water_source_clean | . F_facility_type | F_facility_type | . F_install_year | F_install_year | . management_clean | management_clean | . water_source_category | water_source_category | . water_tech_category | water_tech_category | . Type_of_region | Type_of_region | . Age | Age | . ADM1_EN | ADM1_EN | . UNEMP_CY | UNEMP_CY | . POPPRM_CY | POPPRM_CY | . AVGHHSZ_CY | AVGHHSZ_CY | .   | Cluster_KModes | .   | HDBS_Cluster | .   | Constrained_Multi_Cluster | . Table . Full Random Forest results an all the sampling methods . MCA and clusters Autoencoder and clusters . MCA_0 | AE_0 | . MCA_1 | AE_1 | . Cluster_KModes | Cluster_KModes | . HDBS_Cluster | HDBS_Cluster | . Constrained_Multi_Cluster | Constrained_Multi_Cluster | . Table .Spatial modeling results . Additional Support Plots . . Figure 17 Distribution of population per population point . . Figure KNN vs Error Rate . . Figure HDBSCAN Cluster allocation . . Figure OPTICS Cluster allocation . . Figure OPTICS reachability plot . . Figure .Selection of rural points outside of Urban layer in light blue . . Figure Layer of wetlands in blue and layer of urban areas in pink . . Figure Model 6 classification results using Random Forest undersampled with wet lands layer . . Figure .Model 6 prediction performance table . . Figure . Model 5 prediction performance table . ** ** . Outputs for different models: . Dummy Classifier: . **Same result no matter the sampling** . F1 Score: 0.892 . [[ 0 4941] . [ 0 20486]] . precision Recall f1-score support . 0 0.00 0.00 0.00 4941 . 1 0.81 1.00 0.89 20486 . accuracy 0.81 25427 . macro avg 0.40 0.50 0.45 25427 . weighted avg 0.65 0.81 0.72 25427 . Accuracy: 0.8056790026349943 . Cohen Kappa: 0.000 . AUC: 0.500 . . Naïve Bayes results: . **Sample: Enriched base data, no clusters** . 0.7031895229480474 . [[ 1739 3202] . [ 4345 16141]] . precision Recall f1-score support . 0 0.29 0.35 0.32 4941 . 1 0.83 0.79 0.81 20486 . accuracy 0.70 25427 . macro avg 0.56 0.57 0.56 25427 . weighted avg 0.73 0.70 0.71 25427 . Cohen Kappa: 0.129 . AUC: 0.570 . . **Sample: Enriched base data with clusters** . 0.6943013332284579 . [[ 1787 3154] . [ 4619 15867]] . precision Recall f1-score support . 0 0.28 0.36 0.31 4941 . 1 0.83 0.77 0.80 20486 . accuracy 0.69 25427 . macro avg 0.56 0.57 0.56 25427 . weighted avg 0.73 0.69 0.71 25427 . Cohen Kappa: 0.122 . AUC: 0.568 . . **Sample MCA and clusters** . 0.7954143233570614 . [[ 83 4858] . [ 344 20142]] . precision Recall f1-score support . 0 0.19 0.02 0.03 4941 . 1 0.81 0.98 0.89 20486 . accuracy 0.80 25427 . macro avg 0.50 0.50 0.46 25427 . weighted avg 0.69 0.80 0.72 25427 . Cohen Kappa: 0.000 . AUC: 0.500 . . **Sample autoencoder and clusters** . 0.7932905966099029 . [[ 230 4711] . [ 545 19941]] . precision Recall f1-score support . 0 0.30 0.05 0.08 4941 . 1 0.81 0.97 0.88 20486 . accuracy 0.79 25427 . macro avg 0.55 0.51 0.48 25427 . weighted avg 0.71 0.79 0.73 25427 . Cohen Kappa: 0.029 . AUC: 0.510 . . **Sample traditional oversampling** . 0.6394995708721198 . [[ 3802 5813] . [ 5108 15571]] . precision Recall f1-score support . 0 0.43 0.40 0.41 9615 . 1 0.73 0.75 0.74 20679 . accuracy 0.64 30294 . macro avg 0.58 0.57 0.58 30294 . weighted avg 0.63 0.64 0.64 30294 . Cohen Kappa: 0.151 . AUC: 0.574 . . **Sample X15,y15** . 0.6779014433476226 . [[ 2000 2941] . [ 5249 15237]] . precision Recall f1-score support . 0 0.28 0.40 0.33 4941 . 1 0.84 0.74 0.79 20486 . accuracy 0.68 25427 . macro avg 0.56 0.57 0.56 25427 . weighted avg 0.73 0.68 0.70 25427 . Cohen Kappa: 0.126 . AUC: 0.574 . . Support Vector Machine results: . **Sample: enriched data only** . Accuracy: 0.8093365320328785 . [[ 206 4735] . [ 113 20373]] . precision Recall f1-score support . 0 0.65 0.04 0.08 4941 . 1 0.81 0.99 0.89 20486 . accuracy 0.81 25427 . macro avg 0.73 0.52 0.49 25427 . weighted avg 0.78 0.81 0.74 25427 . Precision: 0.8114146885454835 . Recall: 0.9944840378795274 . Cohen Kappa: 0.056 . AUC: 0.518 . . **Sample enriched and clusters** . Accuracy: 0.8094938451252606 . [[ 170 4771] . [ 73 20413]] . precision Recall f1-score support . 0 0.70 0.03 0.07 4941 . 1 0.81 1.00 0.89 20486 . accuracy 0.81 25427 . macro avg 0.76 0.52 0.48 25427 . weighted avg 0.79 0.81 0.73 25427 . Precision: 0.8105543202033036 . Recall: 0.9964365908425266 . Cohen Kappa: 0.048 . AUC: 0.515 . . **Sample MCA and clusters** . Accuracy: 0.8063475832776182 . [[ 23 4918] . [ 6 20480]] . precision Recall f1-score support . 0 0.79 0.00 0.01 4941 . 1 0.81 1.00 0.89 20486 . accuracy 0.81 25427 . macro avg 0.80 0.50 0.45 25427 . weighted avg 0.80 0.81 0.72 25427 . Precision: 0.8063627057248602 . Recall: 0.9997071170555502 . Cohen Kappa: 0.007 . AUC: 0.502 . . **Sample Autoencoder and clusters** . Accuracy: 0.8061509419121407 . [[ 16 4925] . [ 4 20482]] . precision Recall f1-score support . 0 0.80 0.00 0.01 4941 . 1 0.81 1.00 0.89 20486 . accuracy 0.81 25427 . macro avg 0.80 0.50 0.45 25427 . weighted avg 0.80 0.81 0.72 25427 . Precision: 0.8061557838390995 . Recall: 0.9998047447037001 . Cohen Kappa: 0.005 . AUC: 0.502 . . **Sample traditional oversample** . Accuracy: 0.7245989304812834 . [[ 2690 6925] . [ 1418 19261]] . precision Recall f1-score support . 0 0.65 0.28 0.39 9615 . 1 0.74 0.93 0.82 20679 . accuracy 0.72 30294 . macro avg 0.70 0.61 0.61 30294 . weighted avg 0.71 0.72 0.69 30294 . Precision: 0.7355457114488658 . Recall: 0.9314280187629963 . Cohen Kappa: 0.249 . AUC: 0.606 . . **Sample X15,y15** . Accuracy: 0.7451527903409761 . [[ 2444 2497] . [ 3983 16503]] . precision Recall f1-score support . 0 0.38 0.49 0.43 4941 . 1 0.87 0.81 0.84 20486 . accuracy 0.75 25427 . macro avg 0.62 0.65 0.63 25427 . weighted avg 0.77 0.75 0.76 25427 . Precision: 0.868578947368421 . Recall: 0.8055745387093625 . Cohen Kappa: 0.269 . AUC: 0.650 . . KNN results: . **For k=4 and only enriched data** . [[ 1940 3001] . [ 3101 17385]] . precision Recall f1-score support . 0 0.38 0.39 0.39 4941 . 1 0.85 0.85 0.85 20486 . accuracy 0.76 25427 . macro avg 0.62 0.62 0.62 25427 . weighted avg 0.76 0.76 0.76 25427 . Cohen Kappa: 0.239 . AUC: 0.621 . . **Sample k=4 for enriched with clusters** . [[ 1984 2957] . [ 3158 17328]] . precision Recall f1-score support . 0 0.39 0.40 0.39 4941 . 1 0.85 0.85 0.85 20486 . accuracy 0.76 25427 . macro avg 0.62 0.62 0.62 25427 . weighted avg 0.76 0.76 0.76 25427 . Cohen Kappa: 0.244 . AUC: 0.624 . . **Sample MCA and clusters, k=4** . [[ 1923 3018] . [ 3031 17455]] . precision Recall f1-score support . 0 0.39 0.39 0.39 4941 . 1 0.85 0.85 0.85 20486 . accuracy 0.76 25427 . macro avg 0.62 0.62 0.62 25427 . weighted avg 0.76 0.76 0.76 25427 . Cohen Kappa: 0.241 . AUC: 0.621 . . **Sample Autoencoder and clusters k=3** . [[ 1450 3491] . [ 1958 18528]] . precision Recall f1-score support . 0 0.43 0.29 0.35 4941 . 1 0.84 0.90 0.87 20486 . accuracy 0.79 25427 . macro avg 0.63 0.60 0.61 25427 . weighted avg 0.76 0.79 0.77 25427 . Cohen Kappa: 0.224 . AUC: 0.599 . . **Sample traditional oversampeling** . [[ 5243 4372] . [ 4154 16525]] . precision Recall f1-score support . 0 0.56 0.55 0.55 9615 . 1 0.79 0.80 0.79 20679 . accuracy 0.72 30294 . macro avg 0.67 0.67 0.67 30294 . weighted avg 0.72 0.72 0.72 30294 . Cohen Kappa: 0.347 . AUC: 0.672 . . **Sample X15,y15, k=40** . [[ 2751 2190] . [ 5133 15353]] . precision Recall f1-score support . 0 0.35 0.56 0.43 4941 . 1 0.88 0.75 0.81 20486 . accuracy 0.71 25427 . macro avg 0.61 0.65 0.62 25427 . weighted avg 0.77 0.71 0.73 25427 . Cohen Kappa: 0.250 . AUC: 0.653 . . One-Class SVM results: . **Sample: enriched data only:** . F1 Score: 0.888 . [[ 107 4834] . [ 266 20220]] . precision Recall f1-score support . -1 0.29 0.02 0.04 4941 . 1 0.81 0.99 0.89 20486 . accuracy 0.80 25427 . macro avg 0.55 0.50 0.46 25427 . weighted avg 0.71 0.80 0.72 25427 . Cohen Kappa: 0.013 . AUC: 0.504 . . **Sample enriched and clusters** . F1 Score: 0.889 . [[ 101 4840] . [ 238 20248]] . precision Recall f1-score support . -1 0.30 0.02 0.04 4941 . 1 0.81 0.99 0.89 20486 . accuracy 0.80 25427 . macro avg 0.55 0.50 0.46 25427 . weighted avg 0.71 0.80 0.72 25427 . Cohen Kappa: 0.014 . AUC: 0.504 . . **Sample MCA and clusters*** . F1 Score: 0.889 . [[ 105 4836] . [ 226 20260]] . precision Recall f1-score support . -1 0.32 0.02 0.04 4941 . 1 0.81 0.99 0.89 20486 . accuracy 0.80 25427 . macro avg 0.56 0.51 0.46 25427 . weighted avg 0.71 0.80 0.72 25427 . Cohen Kappa: 0.016 . AUC: 0.505 . . **Sample autoencoder and clusters** . F1 Score: 0.889 . [[ 80 4861] . [ 194 20292]] . precision Recall f1-score support . -1 0.29 0.02 0.03 4941 . 1 0.81 0.99 0.89 20486 . accuracy 0.80 25427 . macro avg 0.55 0.50 0.46 25427 . weighted avg 0.71 0.80 0.72 25427 . Cohen Kappa: 0.010 . AUC: 0.503 . . Isolation Forest results: . **Sample: enriched data only:** . F1 Score: 0.756 . [[ 1724 3217] . [ 6081 14405]] . precision Recall f1-score support . -1 0.22 0.35 0.27 4941 . 1 0.82 0.70 0.76 20486 . accuracy 0.63 25427 . macro avg 0.52 0.53 0.51 25427 . weighted avg 0.70 0.63 0.66 25427 . Cohen Kappa: 0.043 . AUC: 0.526 . . **Sample enriched and clusters** . F1 Score: 0.755 . [[ 1713 3228] . [ 6115 14371]] . precision Recall f1-score support . -1 0.22 0.35 0.27 4941 . 1 0.82 0.70 0.75 20486 . accuracy 0.63 25427 . macro avg 0.52 0.52 0.51 25427 . weighted avg 0.70 0.63 0.66 25427 . Cohen Kappa: 0.039 . AUC: 0.524 . . **Sample MCA and clusters*** . F1 Score: 0.750 . [[ 1633 3308] . [ 6205 14281]] . precision Recall f1-score support . -1 0.21 0.33 0.26 4941 . 1 0.81 0.70 0.75 20486 . accuracy 0.63 25427 . macro avg 0.51 0.51 0.50 25427 . weighted avg 0.69 0.63 0.65 25427 . Cohen Kappa: 0.023 . AUC: 0.514 . . **Sample autoencoder and clusters** . F1 Score: 0.759 . [[ 1789 3152] . [ 6016 14470]] . precision Recall f1-score support . -1 0.23 0.36 0.28 4941 . 1 0.82 0.71 0.76 20486 . accuracy 0.64 25427 . macro avg 0.53 0.53 0.52 25427 . weighted avg 0.71 0.64 0.67 25427 . Cohen Kappa: 0.056 . AUC: 0.534 . . https://water.org/our-impact/where-we-work/uganda/ &#8617; . | https://www.waterpointdata.org/ &#8617; . | https://developers.arcgis.com/documentation/mapping-apis-and-services/demographics/geoenrichment/ &#8617; . | Uganda GeoPortal :https://uganda-africa.hub.arcgis.com/ &#8617; . |",
            "url": "https://www.carlosaordonez.com/fastpages_3/fastpages_3/machine%25learning/2022/03/13/Data-Analysis-and-Prediction-of-Waterpoint-Functionality-Using-Spatial-and-Non-spatial-Machine-Learning-Models.html",
            "relUrl": "/fastpages_3/machine%learning/2022/03/13/Data-Analysis-and-Prediction-of-Waterpoint-Functionality-Using-Spatial-and-Non-spatial-Machine-Learning-Models.html",
            "date": " • Mar 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "How To Build A Crypto Dashboard In Google Sheets",
            "content": "How to build a dashboard for crypto currencies in Google sheets . If you have entered into crypto world, you may have noticed that it is difficult to know what is your exact crypto balance, particularly if you move your crypto out of the exchanges. . What I have read is the importance to manage wallets independently from the exchanges. Not all wallets work for al crypto currencies, and you may be buying from different exchanges. Therefore, it is sometimes difficult to know what is your balance as the crypto may be spread between exchanges and wallets. . Additionally, having an understanding of the value market cap of the different currencies and the circulating supply may give an intuition of the value of the crypto currency. . To accommodate both, the portfolio balance and the market capitalization comparison, I have put together this simple Google Sheet. . This dashboard is meant to provide a single place to track balances based on current prices and give some understanding of the different market caps, from flagged/selected crypto currencies. . Crypto prices are populated from the Crypto Prices API, which provides a simple way to update your dashboard almost a real time. . The API allows to obtain current crypto price by parsing the symbol of the crypto currency. . IMPORTDATA(“https://cryptoprices.cc/BTC/”) . Other data can be called is market capitalization . IMPORTDATA(“https://cryptoprices.cc/BTC/MCAP”) . Also, all time high. . IMPORTDATA(“https://cryptoprices.cc/BTC/ATH”) . Market Capitalization and Circulating Supply . The tab Market_info, contains information to give an insight of the market from a selected number of coins. . On_Off: On(1) Off(0) column is used to select the particular crypto currency, If marked 0 no info is shown. . Symbol: Used for the crypto currency. . Price: Current market price from Data Source . Marketcap: Market capitalization from Data Source . AllTimeHigh: Provides crypto currency all time high . Circulating Supply: Calculated by dividing the market cap by the current price. . Percentage of Circulating Supply from Total Coins: This column calculated the percentage of the circulating supply based on the selected columns. . The Refresh button to the right is macro that executes the update for the Price, Marketcap and AllTimeHigh columns. This link has instructions to help set it up. . The bar plots to the right provides info on the circulating supply vs the market cap, and the percentage of the circulating supply from the selected currencies. . It is important to note that, SHIB and DOGE – particularly SHIB- circulating supply is so large, that it shadows all the other symbols when looking at the percentage of the circulating supply from the total selected coins. . You have the exchanges such as Coinbase and FTX, the wallets. . Gets info on Crypto market . https://coinmarketcap.com/ . Getting started with the macro . https://medium.com/coinmonks/real-time-crypto-prices-on-google-sheets-3-simple-steps-b73d9ba3e81c . Creating the Macro Button . https://spreadsheet.dev/buttons-in-google-sheets#:~:text=To%20assign%20a%20macro%20to,by%20clicking%20on%20the%20button. .",
            "url": "https://www.carlosaordonez.com/fastpages_3/2022/03/12/How-to-build-a-crypto-dashboard-in-Google-Sheets.html",
            "relUrl": "/2022/03/12/How-to-build-a-crypto-dashboard-in-Google-Sheets.html",
            "date": " • Mar 12, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Financial Analysis – Excel Basics",
            "content": "Financial Analysis – Excel Basics . Excel Basics . Change from Manual to Automatic . Sometimes the data could be in manual to avoid error in the formulas. To change it to automatic: . Go to Formulas/Calculation Options/ change to Automatic | . Use of solver . You can use solver to set a specific value depending on the on the desired outcome. . Find the present value to be 100.00 . | Change the interest rate . | Set the objective to be the cell of the present value $B$4 . | By changing variable cells to be the interest rate: $B$1 . | Solve . | Similarly using the constraints by adding the range on which the value could change. . | In this example how to get an A on the last exam . | . . The solver returns 100 for Exam 3 . . Pivot Tables . Use pivot tables to manage large amounts of data. By clicking inside the table Excel understands the limits of the data. . In this case, this data for two campanies, the pivot table shows the average per month . . To group it by quarters, highlight the months, right click/Group/Quarters . . Sensitivity Analysis . Find out how the value of the company changes based on the changes on some of the inputs. . Terminal Value calculates the value in this case from year 4 to infinitive. . . Free cash flows are calculated until year 3 then find terminal value. Add it to year 3, the calculate Present Value (PV) using NPV | . . In order to create a table that shows the variation of the required return and the grow rate, create a table where the values of each in these variables are in row column format with the value of the firm at he corner. Select the data/ Go to Data/ What-if Analysis / Data Table/ Select row input and grow rate constant/ok . . . We can see the table from changes the values on the left side is equal to full table on the right side . . How to create a button . Include the Developer tab. Right click on the ribbon select Developer. . . Insert the radio button | . . Right click/ Format control/Cell link and select a cell. . | If another radio button is created, then value of the cell could toggle between the buttons. In this example value is changing between 1 and 2. If changes, the value changes from 100 to 150 . | . . Hide the cell by changing the color of number 2 to white | . VLookup – Looks up a value vertically. . In this example we are looking for the name and returning how high the dog will jump in feet . .",
            "url": "https://www.carlosaordonez.com/fastpages_3/fastpages_3/financial%20analysis/2022/01/16/Financial-Analysis-Excel-Basics.html",
            "relUrl": "/fastpages_3/financial%20analysis/2022/01/16/Financial-Analysis-Excel-Basics.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Financial Analysis – Balance Sheet, Income Statement, Cash Flow",
            "content": "Balance Sheets, Income Statements, Cash Flow Statements . There are two types of cash flow statements: . Accounting cash flow statements . Reconciles the change in the cash account . | Uses income statement and balance sheets . | . | Financial cash flow . Separates the financing decision form the investment decision . | Cash flow FROM assets . Investment decision | . | Cash flow TO bond/debt holders and stock/equity holders . Financing decision | . | . | Cash is king when we want to get the value or the price of something . | . Income Statement . Follow the accrual method of accounting, income and expenses are recorded in the period that they occurred not in the period cash was received or paid. Remember that income statement does not record the purchase of a fix asset, it records the depreciation expenses, related to the useful life of the asset. . Balance Sheet . It is considered a stock statement. A snapshot of what we have. It is organized in terms of liquidity. Accumulated depreciation is a running total of the individual years worth of depreciation. It is a counter account in the assets column. Liabilities are listed in order of maturity. Accrued expenses, examples is salaries, not paid yet. . Equities: . Preferred stock . | Common stock . Common stock ( $1 par value) - external equity . | Capital surplus – external equity -also known as additional paid in capital. Money paid by investors in excess of par value. If stock sold a $20 but par value at $1 then register $19 difference here. . | . | Accumulated retained earning . Not paid in dividends | . | Less Treasury stock . Counter account , for example when the company re-purchases stock. Buy back stock | . | . Statement of Cash Flows . Example of Flow statement. It shows the sources and uses of cash used by a firm | . . Impact on cash base on this table: . If assets increases cash goes down . | I assets decrease cash goes up . | If liabilities or equity increase cash goes up . | If liabilities or equity decrease cash goes down . | . The statement of cash flows will have the change between the two years . . The Total CF from operating activities include net income, depreciation expenses, increase in deferred taxes and the Operating section. . | For the finance section. Dividends = Net Income – Change in Retained Earnings . | The Proceeds from stock issuance = Change in Common stock + Change Capital surplus. This account shows the stock sold. . | Purchase of stock = Change in less treasury stock / buy back stock . | The sum of total for operating, investing and financing, has to be equal to the change of cash between year in the balance sheet. . | . Financial Cash Flows . . It starts as follows: . . The following shows Cash Flow from Assets: Net Working Capital = Current Assets – Current Liabilities . OCF= Operating Cash Flow . . Remember the claimants own the assets. The assets produced $58MM . . Cash Flow to Debtholders . . If net new borrowing was negative, that will be an inflow to the creditors, the “-“ sign takes care is it is an inflow or an outflow . . Cash flow to equity holders . . For this example: . .",
            "url": "https://www.carlosaordonez.com/fastpages_3/fastpages_3/financial%20analysis/2022/01/16/Financial-Analysis-Balance-Sheet.html",
            "relUrl": "/fastpages_3/financial%20analysis/2022/01/16/Financial-Analysis-Balance-Sheet.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Code - Pandemic Flu Spread Using “Green” Simulation Method for Small Sample of Elementary Students",
            "content": "This paper studies a pandemic flu spread case applying a “green” simulation method, using pseudo-random numbers as presented in the paper by Wilson S., Alabdulkarim A. and Goldsman D. W, “Green Simulation of Pandemic Disease Propagation” in a simulation environment build in Python. The scenario is an elementary school with twenty-one kids and the impact of infection when one infected kid enters the system. The findings and answers to the questions are presented at the end of the paper. The paper can be read here. . Modeling . This section creates the replications. The input here are: The number of kids, the number of days and the probability of infection . replications=100 kids=21 days=30 p=0.02 #initializes the data frame of replications summary_df_replications = pd.DataFrame(data= {&#39;day&#39;:[], &#39;infected&#39;:[], &#39;susceptible&#39;:[],&#39;recovered&#39;:[], &#39;probability&#39;:[], &#39;count_prn&#39;:[] }) #initializes the list of last recovered which I will be saving in a data frame for every iteration summary_last_recovered=summary_df_replications mean_per_day =pd.DataFrame(data= {&#39;day&#39;:[], &#39;infected&#39;:[], &#39;susceptible&#39;:[],&#39;recovered&#39;:[], &#39;probability&#39;:[], &#39;count_prn&#39;:[] }) #runs every iteration for i in range (0, replications): metrics_result, info_last_recovered = pandemic (kids, days+1, p ) #n number of students, k number of days and p probability #in this case 21 students with a lenght of 365 days and probability of 0.02 #creates a single data frame for all replications summary_df_replications=pd.concat([summary_df_replications, pd.DataFrame(data=metrics_result)]) #creates a single data frame for all replications last recovered summary_last_recovered=pd.concat([summary_last_recovered, pd.DataFrame(data=info_last_recovered)]) . def getDayNoInfection(df, infected=&#39;infected&#39;): counter = 0 index = 0 #print(df.head()) for index,d in df.iterrows(): if d[infected] == 0: counter = counter + 1 if counter == 0: index = d[&#39;day&#39;] if counter &gt;= 3: break return index-1 def getDaysOfNoInfectionsMean(df): days_change = [] for i in range(replications): day = getDayNoInfection(df[i*60: i*60 + 60]) days_change.append(day) out= sum(days_change)/len(days_change) #print(out) return (out, days_change) last_infected= getDaysOfNoInfectionsMean(summary_df_replications) #print(counter) . Results . #show the table of summary_df_replications #print(summary_df_replications[0:days]) print (&quot;Table of Replications&quot;) display(summary_df_replications) #testing to see the tail of the entire dataframe #print (summary_df_replications.tail()) #test to see data from the daya of last recovered print (&quot;Last Day of The Iteration per Replication&quot;) display(summary_last_recovered) . Table of Replications . day infected susceptible recovered probability count_prn . 0 1.0 | 1.0 | 20.0 | 0.0 | 0.02 | 20.0 | . 1 2.0 | 1.0 | 20.0 | 0.0 | 0.02 | 40.0 | . 2 3.0 | 1.0 | 20.0 | 0.0 | 0.02 | 60.0 | . 3 4.0 | 0.0 | 20.0 | 1.0 | 0.00 | 80.0 | . 4 5.0 | 0.0 | 20.0 | 1.0 | 0.00 | 100.0 | . ... ... | ... | ... | ... | ... | ... | . 25 26.0 | 0.0 | 14.0 | 7.0 | 0.00 | 393.0 | . 26 27.0 | 0.0 | 14.0 | 7.0 | 0.00 | 407.0 | . 27 28.0 | 0.0 | 14.0 | 7.0 | 0.00 | 421.0 | . 28 29.0 | 0.0 | 14.0 | 7.0 | 0.00 | 435.0 | . 29 30.0 | 0.0 | 14.0 | 7.0 | 0.00 | 449.0 | . 3000 rows × 6 columns . Last Day of The Iteration per Replication . day infected susceptible recovered probability count_prn . 0 30.0 | 0.0 | 20.0 | 1.0 | 0.0 | 600.0 | . 0 30.0 | 0.0 | 17.0 | 4.0 | 0.0 | 520.0 | . 0 30.0 | 0.0 | 6.0 | 15.0 | 0.0 | 318.0 | . 0 30.0 | 0.0 | 7.0 | 14.0 | 0.0 | 329.0 | . 0 30.0 | 0.0 | 13.0 | 8.0 | 0.0 | 446.0 | . ... ... | ... | ... | ... | ... | ... | . 0 30.0 | 0.0 | 20.0 | 1.0 | 0.0 | 600.0 | . 0 30.0 | 0.0 | 16.0 | 5.0 | 0.0 | 492.0 | . 0 30.0 | 0.0 | 15.0 | 6.0 | 0.0 | 472.0 | . 0 30.0 | 0.0 | 6.0 | 15.0 | 0.0 | 303.0 | . 0 30.0 | 0.0 | 14.0 | 7.0 | 0.0 | 449.0 | . 100 rows × 6 columns . summary_last_recovered.mean() . day 30.00 infected 0.00 susceptible 14.03 recovered 6.97 probability 0.00 count_prn 457.01 dtype: float64 . import matplotlib.pyplot as plt import seaborn as sns fig = plt.figure(figsize=(12, 10)) ########## Plotting the first iteration ################# ax = [fig.add_subplot(311, axisbelow=True)] pal = sns.color_palette(&quot;tab10&quot;) max_infected=max(summary_df_replications[&quot;infected&quot;][0:days]) #top plot ax[0].stackplot(summary_df_replications[&quot;day&quot;][0:days], summary_df_replications[&quot;infected&quot;][0:days], summary_df_replications[&quot;susceptible&quot;][0:days], summary_df_replications[&quot;recovered&quot;][0:days], colors=pal, alpha=0.7) ax[0].set_title(f&#39;Susceptible, Infected,and Recoveredwith with {kids} kids, in {days} days, p={p}. First iteration&#39;) ax[0].set_xlabel(&#39;Days&#39;) ax[0].legend([ &#39;Infected&#39;, &#39;Susceptible&#39;, &#39;Recovered&#39;], loc=&#39;best&#39;) ax[0].set_xlim(1, days-1) ax[0].set_ylim(0, kids) #this plots a line with the max infected ax[0].annotate(&quot;Max infected(%.2f)&quot;%(max_infected), (0, max_infected),(0+3, max_infected+3), #(max_infected, max_infected), arrowprops=dict(arrowstyle=&#39;-&gt;&#39;)) ax[0].plot(np.array([0, days-1]), [max_infected, max_infected], lw=3, label=&#39;Max Infected&#39;) . . [&lt;matplotlib.lines.Line2D at 0x16b297baf88&gt;] . summary_df_replications_Table= summary_df_replications.rename(columns={&quot;day&quot;: &quot;Day&quot;,&quot;infected&quot;:&quot;Infected&quot;, &quot;susceptible&quot;:&quot;Susceptible&quot;, &quot;recovered&quot;:&quot;Removed&quot;,&quot;probability&quot;: &quot;Probability&quot;, &quot;count_prn&quot;: &quot;Cumulative PRNs&quot; } ) #summary_df_replications_Table.iloc[0:int(last_infected)] #type summary_df_replications_Table = summary_df_replications_Table.astype({&quot;Day&quot;: int, &quot;Infected&quot;: int}) . ##################### Calculating the Means for each day #################### #calculates the mean per replications #not correcto start mean_replications = pd.DataFrame(summary_last_recovered.mean(axis=0)).T mean_replications # not correcto end #mean of first iteration #print(summary_df_replications[&quot;infected&quot;][0:days].mean()) #mean of the day 1 #print(f&#39;Mean of day 1 :{summary_df_replications[&quot;infected&quot;][0].mean()}&#39;) #mean of all days 1 mean_of_days_infected=[] mean_of_days_susceptible=[] mean_of_days_removed=[] #calculating the variance per day var_of_days_infected=[] var_of_days_susceptible=[] var_of_days_removed=[] #creating a table only for the infected values #if the mean is taking for all of the days including the 0 the mean will also include zero values #set the range of the days x=range(1,days) for i in range(0,days-1): mean_of_days_infected.append(summary_df_replications[&quot;infected&quot;][i].mean()) mean_of_days_susceptible.append(summary_df_replications[&quot;susceptible&quot;][i].mean()) mean_of_days_removed.append(summary_df_replications[&quot;recovered&quot;][i].mean()) #for the variance var_of_days_infected.append(summary_df_replications[&quot;infected&quot;][i].var()) var_of_days_susceptible.append(summary_df_replications[&quot;susceptible&quot;][i].var()) var_of_days_removed.append(summary_df_replications[&quot;recovered&quot;][i].var()) #print(summary_df_replications[&quot;infected&quot;][i].mean()) #print(f&#39;mean of each day {mean_of_days_infected}&#39;) dataframe_means= pd.DataFrame(data={&#39;Day&#39;:x, &#39;mean_per_day_infected&#39;:mean_of_days_infected, &#39;mean_of_days_susceptible&#39;:mean_of_days_susceptible, &#39;mean_of_days_removed&#39;:mean_of_days_removed}) #creates a dataframeof the variances dataframe_variance=pd.DataFrame(data={&#39;Day&#39;:x, &#39;Variance per day Infected&#39;:var_of_days_infected, &#39;Variance per day Susceptible&#39;:var_of_days_susceptible, &#39;Variance per day Recovered&#39;:var_of_days_removed} ) . . last_infected_mean= getDayNoInfection(dataframe_means,&#39;mean_per_day_infected&#39;) print(f&#39;Day that no more kids gets infected {last_infected_mean}th day&#39;) dataframe_means print() mean_days, array_days= getDaysOfNoInfectionsMean(summary_df_replications) End_total_infection = summary_df_replications[summary_df_replications[&quot;infected&quot;]&lt;1] #End_total_infection[] #getDaysOfNoInfectionsMean(summary_df_replications) . Day that no more kids gets infected 21th day . #this plots the info in a stack fig = plt.figure(figsize=(12, 10)) ax = [fig.add_subplot(311, axisbelow=True), fig.add_subplot(312, axisbelow=True)] plt.xticks(np.arange(min(x), max(x)+1, 1.0)) max_infected_means=max(dataframe_means[&quot;mean_per_day_infected&quot;][0:days]) print(max_infected_means) ax[0].stackplot(dataframe_means[&quot;Day&quot;][0:days], dataframe_means[&quot;mean_per_day_infected&quot;][0:days], dataframe_means[&quot;mean_of_days_susceptible&quot;][0:days], dataframe_means[&quot;mean_of_days_removed&quot;][0:days], colors=pal, alpha=0.7) ax[0].set_title(f&#39;Expected Value of Susceptible, Infected,and Recoveredwith with {kids} kids, in {days} days, p={p}, replications= {replications}&#39;) ax[0].set_xlabel(&#39;Days&#39;) ax[0].set_ylabel(&#39;Expected Value per Status&#39;) ax[0].legend([ &#39;Infected&#39;, &#39;Susceptible&#39;, &#39;Recovered&#39;], loc=&#39;center&#39;) ax[0].set_xlim(1, days-2) ax[0].set_ylim(0, kids) #this plots a line with the max infected ax[0].annotate(&quot;Max infected(%.2f)&quot;%(max_infected_means), (0, max_infected_means),(0+3, max_infected_means+3), #(max_infected, max_infected), arrowprops=dict(arrowstyle=&#39;-&gt;&#39;)) ax[0].plot(np.array([0, days-1]), [max_infected_means, max_infected_means], lw=3, label=&#39;Max Infected Means&#39;) #this plots a line with the last day infected ax[0].annotate(&quot;End of Infections(%.2f)&quot;%(last_infected_mean), (last_infected_mean, 0),(last_infected_mean-6, 0+5), #(max_infected, max_infected), arrowprops=dict(arrowstyle=&#39;-&gt;&#39;)) ax[0].annotate(&quot;End of Infections(%.2f)&quot;%(last_infected_mean), (last_infected_mean, 0),(last_infected_mean-6, 0+5), #(max_infected, max_infected), arrowprops=dict(arrowstyle=&#39;-&gt;&#39;)) ax[0].plot(np.array([last_infected_mean,last_infected_mean]), [0, kids], lw=3, label=&#39;Last Infected Means&#39;) ############################################################################################# #This plot is for the infected on the sencond part ax[1].set_xlim(1, days-2) ax[1].set_ylim(0, max_infected_means*3) ax[1].stackplot(x, dataframe_means[&quot;mean_per_day_infected&quot;][0:days-1], dataframe_variance[&quot;Variance per day Infected&quot;][0:days-1], colors=pal, alpha=0.7) ax[1].legend([ &#39;Expected Value of Infected&#39;, &#39;Variance of Infected&#39;], loc=&#39;center&#39;) #this plots a vertical line of end of infection in plot 2 ax[1].annotate(&quot;End of Infections(%.2f)&quot;%(last_infected_mean), (last_infected_mean, 0),(last_infected_mean-6, 0+1), #(max_infected, max_infected), arrowprops=dict(arrowstyle=&#39;-&gt;&#39;)) ax[1].plot(np.array([last_infected_mean,last_infected_mean]), [0, kids], lw=3, label=&#39;Last Infected Means&#39;) #sets x title ax[1].set_xlabel(&#39;Days&#39;) ax[1].set_ylabel(&#39;Expected Value and Variance of Infected&#39;) . . 2.62 . Text(0, 0.5, &#39;Expected Value and Variance of Infected&#39;) .",
            "url": "https://www.carlosaordonez.com/fastpages_3/2022/01/04/Pandemic-simulation-Covid-First-Version.html",
            "relUrl": "/2022/01/04/Pandemic-simulation-Covid-First-Version.html",
            "date": " • Jan 4, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Pandemic Flu Spread Using “Green” Simulation Method for Small Sample of Elementary Students",
            "content": "Authors: Carlos Ordonez and Juan Carlos Pineda . Abstract: This paper studies a pandemic flu spread case applying a “green” simulation method, using pseudo-random numbers as presented in the paper by Wilson S., Alabdulkarim A. and Goldsman D. W, “Green Simulation of Pandemic Disease Propagation” (Wilson S., 2019) in a simulation environment built in Python. The scenario is an elementary school with twenty-one kids and the impact of infection when one infected kid enters the system. The findings and answers to the questions are presented in this paper. . Background and Description of the Problem | The novel Coronavirus pandemic has affected how the world operates, limiting our ability to mobilize and interact within each other. Throughout the history of humankind there have been several endemic and pandemic viruses that have attacked our society (Morens DM, 2009;200(7)), and researchers have study and implemented models to understand how the viruses spread among the population. For this paper the susceptible-infected-removed (SIR) model is used. The SIR model states that an individual at time t could change between three different states, susceptible (S), infected (I), and removed (R) (Morens DM, 2009;200(7)) (Munkhbat, 2019). . This paper analyzes the case of an elementary school with twenty-one students having one infected child on the first day. The infection rate β = 3 as the infected kid can infect on three consecutive days. The probability of infecting any other kid is p=0.02 and it is modeled using Bernoulli independent, identically distributed p trials. As a new kid becomes infected, he or she will have the same 3-day infection rate and the same p probability to infect other children. . For this simulation we will be using a simplified approached not considering any mitigation effect through social distance, or more complex models. The simplified approach is based on the “green” method presented in the paper by Wilson (Wilson S., 2019) that reuses many Bernoulli p trials between runs of susceptible individuals. The first section describes the setup of the environment. The second section describes the result with different iterations and answers to the questions such as the distribution of infection of Day1 , the expected number of kids infected on Day 1, the expected number of kids infected by Day 2 and a histogram detailing the length of the pandemic. . Main Findings . Modeling | | The SIR model is a mechanistic description of the spread of infectious disease. Each parameter is a state defined by differential equations that model the observed data at any given time t. The state susceptible refer to the individual that could be exposed to an infected person. Infected is the person that acquired the disease. Removal are individuals that either recovered or dead. . . Figure 1. SIR Model block diagram . The differential equations 1a,1b, 1c describes each of the models from the Figure 1. . . Susceptible individuals will transition to the infected state at a rate of β and then will continue transition to the removed at a rate δ. This model is widely used in large populations. As presented by Wilson (Wilson S., 2019), the use of differential equation model tends to perform poorly in a setting with a small population. Also, he points out that additional short comings of this model do not take randomness into account and individual level behavior. . A better approach as Wilson et al. (Wilson S., 2019), proposed, is the use of discrete-event simulation, using a “green” simulation technique, where individuals enter in contact with each other and with the infected individual, and the outputs from the results of the previous days are being reused. This technique was introduced by Staum et. al (Feng &amp; Staum, 2015), and fits the model of the 21 elementary students in the project. Using a simplified version of the SIR model, we are simulating susceptible random individuals that may be infected by “Tommy”, the infected individual, at most once per day. Having a probability of infection p, each encounter being independent, therefore meeting the requirement of having encounters as Bernoulli trials. The simulation will end when no more susceptible individuals will get infected or no more susceptible individual will remain. The simulation runs for different sets of independent replications to calculate the expected number of kids infected, and to answer the questions presented in the project. . Setup of the Simulation Environment | The simulation starts by generating uniform - Unif (0,1) - pseudo-random numbers (PRN) U for each susceptible individual for each day per each replication. Twenty (20) PRNs are created for each susceptible kid on Day 1 and compared to the probability of infection p = 0.02. There is one super-infective kid, “Tommy”, which is represented as I(0)=1 arriving to the school on the first day. The number of susceptible at the beginning of Day 1 is represented by S(0)=20 and the number of removals R(0)=0. Tsai, et al. (Tsai, et al., 2010) proposed a way that all the k infectives entering the room, interacts with all n susceptible, by means of Bern(p) trials. Then, the probability of a susceptible becoming infected is equal to 1 - qk with q = 1 – p. Therefore, performing n Bern(1 - qk). For example, on Day 1 the probability of p*(t=1) = 1-(1 - 0.02)1 = 0.02. It is important here to highlight, that as each day passes the probability changes depending on the number of k infected kids. The simulation will have a Bern(p*(t)) trials and will end when no more kids are infected. The number of k infected kids per day is calculated by comparing p*(t) &lt; U and updating the value of p* every day. On Day 2 PRNs are calculated for the remaining susceptible children and so on until no additional kid gets infected. To give another example, in order to calculate Day 2, assuming that result of the PRN for the end of the Day 1 is 2 infected, p = 0.02, the Day 2 will start with S(2) = 19, I(2)=2 including “Tommy”, p*(2)= 1 - ( 1 - 0.02)2 = 0.0396, R(0)= 0 and so on. The problem establishes that infected individuals will remain infected for a δ of three days, therefore “Tommy” will change from “infected” to “removed” on Day 4, for our example R(4)=2. . Modeling and Results | Table 1 shows how the states from the SIR model changes, the probability and the number of cumulative PRNs generated from Day 1 until no additional children gets infected for the first replication. On Day 1 there is 1 infected, “Tommy” = I(1), with S(1) = 20 susceptibles, R(1) = 0 recovered, a probability p*(1)=0.02, with total number of PRNs generated, and a total number of PRNs= 20 (Cumulative PRNs) . Day 2 shows s one additional infected for a total of I(2) =2, S(2)= 19, R(2) = 0, the new p*(2)=0.0396, total cumulative PRNs= 40. On Day 4 we see the first removed, “Tommy”. As the days progress, we stop seeing new infections Day 6 and the last infected person gets removed on Day . Therefore the spread of infection ends on Day 10 with a total infected of I(10)=9 kids and S(10)=12 susceptible that did not get infected. | . Figure 2 shows the first replication using a stack plot where can be seen how the infection susceptible and removed individuals change status over time for the 21 kids with a 3 day infection rate, and a probability of infection of p=0.02. . The peak of the of the number of infected individuals happened on Day 5. On Day 6 started to die down until the spread stopped on Day 9 where the last kid was removed or recovered. The total of susceptible/not-infected individuals were 12, and the total of infected individuals were 9. . . Figure 2. Stack plot of 21 kids with 30 days for the first replication . Figure 3, shows the next step, running 100 replications of this same scenario and finding the expected value of the status per day to understand how the model behaves using the same base parameters. The top part shows the stacked plot of the expected value of each day for the different status. On the bottom part of the same figure is an expanded view of the rate of infections and the variance. The figure shows that on average the day with most infections is Day 3. Additionally, it shows that on average the infections end on Day 24. Table 4 on the Appendix section shows the value of each day for the different status (infected, susceptible, and removed) . . Figure 3. Top: Expected value of status per day with 21 kids in 30 days p=0.02 and 100 replications. Bottom : Expected value and variance of infected per day with 21 kids in 30 days p=0.02 and 100 replications . Taking the expected value for each of the three status after 100 replications, in average 7.13% of students get infected, 13.87% of students remain susceptible, and 7.13% gets removed. Table 2 presents the restuls. We can imply that about 7 kids get infected and 14 kids do not. The variance as seen in orange on the bottom graph of Figure 3, has it highest pick on Day 6 with a pick of 5.15 and expected value of 1.77 , implying that number of kids infected for that Day 6 could change between 2 to 5. This could be the result of having a three (3) day infection rate. . . The following histograms on Figure 4 shows where most of values fall after the 100 replications. It can be seen that at least one person gets infected most of the time, which is obvious since the system is initialize with Tommy, the infected kid. After that, it varies between two to six following what looks like a geometric distribution considering that we are doing the number of Bernoulli trials until the first success. The recovered distribution seems to follow also a geometric distribution and the susceptible values complement recovered ones. . . Figure 4. Histograms of infected, recovered/removed, susceptible and the probability after 100 replications . To validate that the values for Day 1 corresponded to a geometrically distribution, we performed a goodness of fit with a group of smaller values of n=60 for replications 1 to 990 in batches, with a total number of batches equals to 14 batches. We encountered that 11 times out of 14 our χo was smaller than the χα at 0.95. Resulting that it is very likely that the distribution for the first day fits the geometric. Results of the 14th batch is shown below. Table 5 in the Appendix shows the total results per batch. . . Figure 5. 14th Batch results for Goodness of fit. . Answer to the questions presented in the Project . What is the distribution of the number of kids that Tommy infects on Day 1? | | After taking the values of m=100 replications and plotting the results, we realized there are at least one kit kid gets infected 69% of the time. Similarly, at least 2 kids get infected 21% of the time and at least 3 kids get infected 10% of the time for the 100 replications reflected on Table 2. . . . Therefore, the distribution for Day 1 as presented in the histogram of Figure 3 shows a geometric distribution. . . Figure 6. Distribution for Day 1 . What is the expected number of kids that Tommy infects on Day 1? | The expected number of kids that Tommy infects on Day 1 can be defined: . . The result is equal to 1.48. . What is the expected number of kids that are infected by Day 2 (you can count Tommy if you want)? | Similarly, the expected number of kids that are infected on Day 2 is equal to 2.07 . . Simulate the number of kids that are infected on Days 1,2,. . . . Do this many times. What are the (estimated) expected numbers of kids that are infected by Day i, i = 1, 2, . . .? Produce a histogram detailing how long the “epidemic” will last. | . Figure 7. Histogram of expected value per status . After running the simulation for 100 times (m=100) and taking the expected value for each status per day, we see that the infected values approximate to a geometric distribution. As can be seen in the histograms below on Figure 6. The Expected Value of Susceptible and Expected Value of Infected kids are likely to follow a geometric distribution. The Expected value of the removed roughly fits a skewed normal distribution, which is an encouraging result as is similar to the one presented by Disworh (Disworth, 2019). . The values of the Expected Value of removed kids and the Expected value of the infected can be seen as a “mirror” because as the days progress, the kids that get infected become removed. . Conclusions | After reading all the material on Pandemic and/or Disease transmission we learned the way how these problems are modeled. SEIR and SIR models are used to capture the process of the infection. The SEIR model is more general, however for our solution we picked the SIR model as described by Wilson et al (Wilson S., 2019) due to its simplicity way of modeling by means of Bernoulli process. . One key takeaway of the simulation is that even though the model is simple, implementing it has to be done carefully since generating PRN’s in a large number of replications can be costly from the computing stand point. This Is very important and we noticed it after growing our replication number from 10, to 100 to 1,000 and then further from that. . Once we determined the right length of the spread of the virus, the “green” simulation technique used for this project becomes computationally efficient. The “green” simulation technique considers a probability that reuses many Bernoulli p trials between runs of susceptible individuals, updating the rate of infected induvial between consecutive days. The results showed that in average the pandemic lasted k= 24 days after m= 100 replications, with n= 20 susceptible kids exposed to 1 infected on Day 1. It also resulted that the total number of expected kids I(t) = 7 infected a including Tommy; and the total S(t) =14 susceptible kids, after some rounding. The distribution of the infected individuals after the m=100 replication for Day 1 is likely to be geometric. The same is the case for the distribution of the expected values of infected kids. The variance peaked on Day 6, which could be due to the three days that infected kids continue spreading the virus. . Finally, this simulation shows a simplified way to get acquainted with using PRNs to estimate the rate of infections. Further work could include factoring the use of social distance or other means that could help mitigate the infection rate. Portions of the code can be found here. . References . Disworth, M. (2019, January 2). Infecting Modeling - Part 1. (Towards Data Science) Retrieved from https://towardsdatascience.com/infection-modeling-part-1-87e74645568a . Feng, M., &amp; Staum, J. (2015). Green simulation designs for repeated experiments. In Proceedings of the 2015 Winter. . Morens DM, F. G. ( 2009;200(7)). What is a pandemic? . J Infect Dis., 1018-1021. . Munkhbat, B. (2019). A Computational Simulation Model for Predicting Infectious Disease Spread using the Evolving Contact Network Algorithm. Master Thesis. . Tsai, M., Chern, T., Chuang, J., Hsueh, C., Kuo, H., Liau, C., . . . Shen, C. (2010). Efficient simulation of the spatial transmission dynamics of influenza. PLoS ONE, 5, e13292. . Wilson S., A. A. ( 2019). Simulation of Pandemic Disease Propagation . Symmetry, 11(4), 580. . Appendix . . .",
            "url": "https://www.carlosaordonez.com/fastpages_3/simulation/2021/05/16/Pandemic-Flu-Spread-Using-Green-Simulation-Method-for-Small-Sample-of-Elementary-Students.html",
            "relUrl": "/simulation/2021/05/16/Pandemic-Flu-Spread-Using-Green-Simulation-Method-for-Small-Sample-of-Elementary-Students.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Calculating The Cost Of Overbooking Airline Tickets Using Analytics",
            "content": "This article was originally posted on Linkedin here prior to the pandemic Using Python to calculate the cost of overbooking an airline ticket. . The code can be found in GitHub here. . I have been curious to use data analytics in aviation operations. I found that one of the models that airlines use is the binomial function to determine what is the optimum revenue the airline, after calculating the probability of the number of passengers that will show up when overbooking a specific route. The probability is modeled with binomial function assuming that each show or no show sample is independent of each other. This probability is affected by many possible factors like delayed flights, traffic jams, canceling a flight due to personal reasons, etc. . Binomial function This article was inspired by Cory Simon ’s article on “ By How Many Flights an Airline Should Overbook”. In his analysis, he uses a normal approximation of a binomial distribution. He allocates fixed parameters such as a number of seats per aircraft, the probability of a passenger and expands to a range of additional tickets beyond capacity sold the price per ticket and the price of the voucher. The voucher in his example is defined as the cost the airline incurs to pay passengers for an overbooked ticket. . I decided to expand a little more and create a function that could allow passing the value of the ticket, the voucher cost, the number of seats, the probability rate of a passenger showing up and the number of additional seats. I used Python’s numpy cumulative distributive function to calculate the probability of passengers to show to a flight beyond capacity (Total Seats Available in the aircraft + total additional overbooking tickets). The function also calculates the max number of possible seats available that could be overbooked. This is defined by p * x=Total Seats Available. If the probability is equal to 1 then all seats will be taken. By solving x, then x=Total Seats Available/p _will give the maximum seats available for that probability _p. A good explanation of the calculation of the probability of overbooking can also be found here. . Let’s assume the case of  Embraer 175  flying  Miami to Cleveland with 128 seats  in the main cabin. . Embraer-175-cabin-Layout For this calculation, I am focused on the number of additional tickets beyond capacity. The following example tests a ticket with a value of $512.00 from Miami to Cleveland in American Airlines, assuming a probability rate of passengers to show at this time on this date at 90%. The maximum number of additional seats the airline could sell is equal to_ Max_Seats = 128 /.9 = 142.22 - 128 = 14.22 _rounded down will give  14  additional seats on  128  seat aircraft, with a probability of  90 % of the passengers showing up and a voucher cost for overbooking of  $200.00. . Air Fare example for Binomial Function The airline by selling additional tickets increases its revenue, but the net profit decreases if they end up paying for too many vouchers. The voucher cost could include the cost of re-scheduling passengers. In this example, it is omitted. I am trying to find the optimum amount of additional tickets the airline could sell. . Overbooking Calculation image The result of this graph shows the maximum profit per additional ticket at a fixed voucher cost, with the maximum amount of overbooking tickets (14). The maximum total profit can be reached at $66,995.94 overbooking 6 tickets of the 14. . Expected Reveneu vs tickets beyond capacity graph Table with Expected profit by exceeding overbooking The following table test different vouchers values based on the suggested overbooking tickets found on the previous table. . Table with Overbooking tickets sold and max profit The table shows that the ideal  Voucher_Cost  should be less than $200.00, overbooking 6 seats. If the voucher cost offered is higher than $600.00 the airline should not overbook any additional tickets. . This analysis is just the first to understand a model, in this case, the binomial function, when applying analytics in aviation operations. It is important to highlight that there are 4 moving variables: Ticket sale price, the voucher cost, probability of passengers to show at a specific time and date, and the number of seats per aircraft. .",
            "url": "https://www.carlosaordonez.com/fastpages_3/2020/08/15/calculating-the-cost-of-overbooking-airline/",
            "relUrl": "/2020/08/15/calculating-the-cost-of-overbooking-airline/",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.carlosaordonez.com/fastpages_3/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.carlosaordonez.com/fastpages_3/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an electrical engineer, data scientist, business focused, helping companies adopt a digital mindset. . To learn more about me, check my LinkedIn. .",
          "url": "https://www.carlosaordonez.com/fastpages_3/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.carlosaordonez.com/fastpages_3/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}